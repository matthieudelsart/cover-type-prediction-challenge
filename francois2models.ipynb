{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.calibration import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2MODELS\n",
    "Predictions based on 2 models\n",
    "1. Classify between 1-2 and 3-4-5-6-7\n",
    "2. For each class, find the cover_Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def split_in_two_df(df: pd.DataFrame):\n",
    "    if \"under2\" not in df.columns:\n",
    "        # Split between cover type >2 and <=2\n",
    "        df[\"under2\"] = (df[\"Cover_Type\"] < 3).astype(int)\n",
    "    \n",
    "    # 1 -2\n",
    "    df_under = df.where(df[\"under2\"] == 1)\n",
    "    df_under.dropna(inplace=True)\n",
    "    df_under = df_under.astype(int)  # @TODO c'est bizarre\n",
    "\n",
    "    # 3-4-5-6-7\n",
    "    df_above = df.where(df[\"under2\"] == 0)\n",
    "    df_above.dropna(inplace=True)\n",
    "    df_above = df_above.astype(int)  # @TODO c'est bizarre\n",
    "\n",
    "    return df, df_under, df_above\n",
    "\n",
    "\n",
    "def get_X_y(df: pd.DataFrame, target_col=None):\n",
    "    X = df.copy()\n",
    "    if target_col == \"under2\":\n",
    "        X.drop([\"Cover_Type\"], axis=1, inplace=True)\n",
    "    elif target_col == \"Cover_Type\":\n",
    "        X.drop([\"under2\"], axis=1, inplace=True)\n",
    "    if target_col in X.columns:\n",
    "        y = X[target_col]\n",
    "        X.drop([target_col], axis=1, inplace=True)\n",
    "        return X, y\n",
    "    else:\n",
    "        # X.drop([\"under2\", \"Cover_Type\"], axis=1, inplace=True)\n",
    "        return X\n",
    "\n",
    "\n",
    "def train_first_model(X, y, model, test_size=0.25, seed=42, startify=True):\n",
    "    if startify:\n",
    "        strf = y\n",
    "    else:\n",
    "        strf = None\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=strf)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    # model.fit(X_train, y_train)\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_second_model(X, y, model, test_size=0.25, seed=42, startify=True):\n",
    "    if startify:\n",
    "        strf = y\n",
    "    else:\n",
    "        strf = None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=strf)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    model.fit(X, le.fit_transform(y))\n",
    "    # model.fit(X_train, y_train)\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred = le.inverse_transform(y_pred)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Load training data\n",
    "df_train = get_data_train()\n",
    "\n",
    "# Split the data\n",
    "df_train, df_train_under, df_train_above = split_in_two_df(df_train)\n",
    "X_train_under, y_train_under = get_X_y(\n",
    "    df_train_under, target_col='Cover_Type')\n",
    "X_train_above, y_train_above = get_X_y(\n",
    "    df_train_above, target_col='Cover_Type')\n",
    "X_train, y_train = get_X_y(df_train, target_col='under2')\n",
    "\n",
    "# Train first model\n",
    "model = AdaBoostClassifier(n_estimators=300, algorithm='SAMME')#xgb.XGBClassifier(n_estimators=200, max_depth=15)\n",
    "model1 = train_first_model(X_train, y_train, model)\n",
    "\n",
    "# Train second model\n",
    "model2u = AdaBoostClassifier(n_estimators=300, algorithm='SAMME')#xgb.XGBClassifier(n_estimators=200, max_depth=15)\n",
    "model2u = train_second_model(\n",
    "    X_train_under, y_train_under, model2u)\n",
    "\n",
    "model2a = xgb.XGBClassifier(n_estimators=200, max_depth=15, objective=\"multi:softmax\")\n",
    "model2a = train_second_model(\n",
    "    X_train_above, y_train_above, model2a)\n",
    "\n",
    "# Load Testing data\n",
    "df_test = get_data_test()\n",
    "\n",
    "X_test = get_X_y(df_test)\n",
    "y1_pred = model1.predict(X_test)\n",
    "\n",
    "X_test[\"under2\"] = y1_pred\n",
    "\n",
    "df_test1, df_test_under, df_test_above = split_in_two_df(X_test)\n",
    "X_test_under = get_X_y(df_test_under)\n",
    "X_test_under.drop([\"under2\"], axis=1, inplace=True)\n",
    "X_test_above = get_X_y(df_test_above)\n",
    "X_test_above.drop([\"under2\"], axis=1, inplace=True)\n",
    "\n",
    "y2u_pred = model2u.predict(X_test_under)\n",
    "y2a_pred = model2a.predict(X_test_above)\n",
    "\n",
    "# Convert prediction into Dataframe\n",
    "pred_under = pd.DataFrame({'Id': X_test_under.Id, 'Cover_Type': y2u_pred+1})\n",
    "pred_above = pd.DataFrame({'Id': X_test_above.Id, 'Cover_Type': y2a_pred+3})\n",
    "\n",
    "final_pred = pd.concat([pred_under, pred_above])\n",
    "final_pred_clean = clean_predictor(y_pred=final_pred.Cover_Type, Id=final_pred.Id)\n",
    "csv_for_submission(final_pred_clean,\"2model+clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice the best model for first split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92662037, 0.97048611, 0.54305556, 0.90300926, 0.43726852])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load training data\n",
    "df_train = get_data_train()\n",
    "\n",
    "# Split the data\n",
    "df_train, df_train_under, df_train_above = split_in_two_df(df_train)\n",
    "\n",
    "X_train, y_train = get_X_y(df_train, target_col='under2')\n",
    "\n",
    "# Train first model\n",
    "model = xgb.XGBClassifier(n_estimators=200, max_depth=15)\n",
    "\n",
    "cross_val_score(model, X_train, y_train, cv =5, scoring=\"balanced_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# models = [xgb.XGBClassifier(n_estimators=500), xgb.XGBRFClassifier(n_estimators=500), RandomForestClassifier(n_estimators=500), AdaBoostClassifier(n_estimators=500, algorithm='SAMME')]\n",
    "models=[AdaBoostClassifier(n_estimators=1000, algorithm='SAMME')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME', n_estimators=1000)\n",
      "Mean 0.7262268518518519\n",
      "std 0.21902431849433232\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    scores = cross_val_score(m, X_train, y_train, cv=5, scoring=\"balanced_accuracy\")\n",
    "    print(m)\n",
    "    print('Mean', np.mean(scores))\n",
    "    print('std', np.std(scores))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n",
      "Mean 0.6853174603174603\n",
      "std 0.3106491830859837\n",
      "\n",
      "\n",
      "XGBRFClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                colsample_bylevel=None, colsample_bytree=None, device=None,\n",
      "                early_stopping_rounds=None, enable_categorical=False,\n",
      "                eval_metric=None, feature_types=None, gamma=None,\n",
      "                grow_policy=None, importance_type=None,\n",
      "                interaction_constraints=None, max_bin=None,\n",
      "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "                multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "                num_parallel_tree=None, objective='binary:logistic',\n",
      "                random_state=None, reg_alpha=None, ...)\n",
      "Mean 0.663425925925926\n",
      "std 0.3182901137073865\n",
      "\n",
      "\n",
      "RandomForestClassifier(n_estimators=500)\n",
      "Mean 0.6855820105820105\n",
      "std 0.31739364930480196\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(n_estimators=500)\n",
      "Mean 0.6956349206349206\n",
      "std 0.2963235029789866\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    scores = cross_val_score(m, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    print(m)\n",
    "    print('Mean', np.mean(scores))\n",
    "    print('std', np.std(scores))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2MODELS with lGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.3105357548553214\n",
      "2: 0.4132358712040371\n",
      "3: 0.06796761512670994\n",
      "4: 0.0070549317397919495\n",
      "5: 0.06873696240353039\n",
      "6: 0.04419185834371751\n",
      "7: 0.0882770063268917\n",
      "#\n",
      "3: 0.24605587817461305\n",
      "4: 0.025540213842434514\n",
      "5: 0.24884106372903322\n",
      "6: 0.15998305211474714\n",
      "7: 0.31957979213917204\n",
      "#\n",
      "3: 0.14480771627246217\n",
      "4: 0.03620192906811554\n",
      "5: 0.14480771627246217\n",
      "6: 0.14480771627246217\n",
      "7: 0.14480771627246217\n",
      "#\n",
      "12 420520\n",
      "3456 160492\n",
      "ratio 2.6201929068115546\n",
      "#\n",
      "1: 180425\n",
      "2: 0.4132358712040371\n",
      "ratio 1.3307191353748096\n"
     ]
    }
   ],
   "source": [
    "print(\"1:\",180425/581012)\n",
    "print(\"2:\",240095/581012)\n",
    "print(\"3:\",39490/581012)\n",
    "print(\"4:\",4099/581012)\n",
    "print(\"5:\",39937/581012)\n",
    "print(\"6:\",25676/581012)\n",
    "print(\"7:\",51290/581012)\n",
    "\n",
    "a = 581012 - 180425 -240095\n",
    "print(\"#\")\n",
    "print(\"3:\",39490/a)\n",
    "print(\"4:\",4099/a)\n",
    "print(\"5:\",39937/a)\n",
    "print(\"6:\",25676/a)\n",
    "print(\"7:\",51290/a)\n",
    "print(\"#\")\n",
    "print(\"3:\",0.04*581012/a)\n",
    "print(\"4:\",0.01*581012/a)\n",
    "print(\"5:\",0.04*581012/a)\n",
    "print(\"6:\",0.04*581012/a)\n",
    "print(\"7:\",0.04*581012/a)\n",
    "print(\"#\")\n",
    "print('12', 180425+240095)\n",
    "print('3456', 39490+4099+39937+25676+51290)\n",
    "print('ratio',(180425+240095)/(39490+4099+39937+25676+51290))\n",
    "print(\"#\")\n",
    "print(\"1:\",180425)\n",
    "print(\"2:\",240095/581012)\n",
    "print('ratio',240095/180425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "class_weights_matt = {\n",
    "    1: 0.4,\n",
    "    2: 0.45,\n",
    "    3: 0.04,\n",
    "    4: 0.01,\n",
    "    5: 0.04,\n",
    "    6: 0.04,\n",
    "    7: 0.04\n",
    "}\n",
    "\n",
    "c_weights_bin = {\n",
    "    1: 0.85,\n",
    "    0: 0.17\n",
    "}\n",
    "\n",
    "c_weights_12 = {\n",
    "    1: 0.5,\n",
    "    2: 0.5\n",
    "}\n",
    "\n",
    "c_weights_34567 = {\n",
    "    3: 0.15,\n",
    "    4: 0.03,\n",
    "    5: 0.15,\n",
    "    6: 0.15,\n",
    "    7: 0.15\n",
    "}\n",
    "\n",
    "coeffs = np.array([2.63, 3.06, 0.43, 0.05, 0.24, 0.27, 0.32])\n",
    "\n",
    "\n",
    "\n",
    "model1 = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    # num_iterations=100,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model2u = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    # num_iterations=100,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model2a = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=5,\n",
    "    # class_weight=c_weights_34567,\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    # num_iterations=100,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_data_train()\n",
    "df_train, df_test = train_test_split(df_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Cover_Type</th>\n",
       "      <th>Soil_Type_Synth</th>\n",
       "      <th>Wilderness_Area_Synth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242642</td>\n",
       "      <td>2881</td>\n",
       "      <td>130</td>\n",
       "      <td>22</td>\n",
       "      <td>210</td>\n",
       "      <td>54</td>\n",
       "      <td>1020</td>\n",
       "      <td>250</td>\n",
       "      <td>221</td>\n",
       "      <td>88</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309891</td>\n",
       "      <td>3005</td>\n",
       "      <td>351</td>\n",
       "      <td>14</td>\n",
       "      <td>242</td>\n",
       "      <td>-16</td>\n",
       "      <td>1371</td>\n",
       "      <td>194</td>\n",
       "      <td>215</td>\n",
       "      <td>159</td>\n",
       "      <td>842</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287847</td>\n",
       "      <td>3226</td>\n",
       "      <td>63</td>\n",
       "      <td>14</td>\n",
       "      <td>618</td>\n",
       "      <td>2</td>\n",
       "      <td>1092</td>\n",
       "      <td>232</td>\n",
       "      <td>210</td>\n",
       "      <td>107</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>516307</td>\n",
       "      <td>3298</td>\n",
       "      <td>317</td>\n",
       "      <td>8</td>\n",
       "      <td>661</td>\n",
       "      <td>60</td>\n",
       "      <td>752</td>\n",
       "      <td>198</td>\n",
       "      <td>233</td>\n",
       "      <td>174</td>\n",
       "      <td>1248</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124860</td>\n",
       "      <td>3080</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>175</td>\n",
       "      <td>26</td>\n",
       "      <td>3705</td>\n",
       "      <td>219</td>\n",
       "      <td>227</td>\n",
       "      <td>144</td>\n",
       "      <td>2673</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15115</th>\n",
       "      <td>475155</td>\n",
       "      <td>3328</td>\n",
       "      <td>321</td>\n",
       "      <td>13</td>\n",
       "      <td>323</td>\n",
       "      <td>12</td>\n",
       "      <td>5109</td>\n",
       "      <td>186</td>\n",
       "      <td>227</td>\n",
       "      <td>180</td>\n",
       "      <td>3151</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15116</th>\n",
       "      <td>514378</td>\n",
       "      <td>3455</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>841</td>\n",
       "      <td>92</td>\n",
       "      <td>939</td>\n",
       "      <td>220</td>\n",
       "      <td>229</td>\n",
       "      <td>146</td>\n",
       "      <td>362</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15117</th>\n",
       "      <td>368425</td>\n",
       "      <td>3279</td>\n",
       "      <td>90</td>\n",
       "      <td>14</td>\n",
       "      <td>404</td>\n",
       "      <td>113</td>\n",
       "      <td>1513</td>\n",
       "      <td>240</td>\n",
       "      <td>218</td>\n",
       "      <td>105</td>\n",
       "      <td>1503</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15118</th>\n",
       "      <td>537844</td>\n",
       "      <td>3589</td>\n",
       "      <td>357</td>\n",
       "      <td>9</td>\n",
       "      <td>418</td>\n",
       "      <td>52</td>\n",
       "      <td>1868</td>\n",
       "      <td>205</td>\n",
       "      <td>223</td>\n",
       "      <td>155</td>\n",
       "      <td>1657</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15119</th>\n",
       "      <td>463634</td>\n",
       "      <td>3385</td>\n",
       "      <td>345</td>\n",
       "      <td>15</td>\n",
       "      <td>350</td>\n",
       "      <td>76</td>\n",
       "      <td>3625</td>\n",
       "      <td>190</td>\n",
       "      <td>216</td>\n",
       "      <td>164</td>\n",
       "      <td>3327</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15120 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  Elevation  ...  Soil_Type_Synth  Wilderness_Area_Synth\n",
       "0      242642       2881  ...               30                      1\n",
       "1      309891       3005  ...               24                      3\n",
       "2      287847       3226  ...               29                      1\n",
       "3      516307       3298  ...               23                      2\n",
       "4      124860       3080  ...               24                      1\n",
       "...       ...        ...  ...              ...                    ...\n",
       "15115  475155       3328  ...               38                      3\n",
       "15116  514378       3455  ...               40                      2\n",
       "15117  368425       3279  ...               29                      1\n",
       "15118  537844       3589  ...               40                      2\n",
       "15119  463634       3385  ...               40                      3\n",
       "\n",
       "[15120 rows x 14 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = get_data_train()\n",
    "preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 13)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [   593   3024 134298]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 172\u001b[0m\n\u001b[1;32m    169\u001b[0m X_test_above\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munder2\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m y2u_pred \u001b[38;5;241m=\u001b[39m model2u\u001b[38;5;241m.\u001b[39mpredict(X_test_under)\n\u001b[0;32m--> 172\u001b[0m y2a_pred \u001b[38;5;241m=\u001b[39m model2a\u001b[38;5;241m.\u001b[39mpredict(X_test_above)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Convert prediction into Dataframe\u001b[39;00m\n\u001b[1;32m    175\u001b[0m pred_under \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m: X_test_under\u001b[38;5;241m.\u001b[39mId, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCover_Type\u001b[39m\u001b[38;5;124m'\u001b[39m: y2u_pred})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/sklearn.py:1192\u001b[0m, in \u001b[0;36mLGBMClassifier.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     class_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(result, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39minverse_transform(class_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:160\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    158\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(y, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)))\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n\u001b[1;32m    161\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[y]\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [   593   3024 134298]"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils import *\n",
    "\n",
    "def split_in_two_df(df: pd.DataFrame):\n",
    "    if \"under2\" not in df.columns:\n",
    "        # Split between cover type >2 and <=2\n",
    "        df[\"under2\"] = (df[\"Cover_Type\"] < 3).astype(int)\n",
    "    \n",
    "    # 1 -2\n",
    "    df_under = df.where(df[\"under2\"] == 1)\n",
    "    df_under.dropna(inplace=True)\n",
    "    df_under = df_under.astype(int)  # @TODO c'est bizarre\n",
    "\n",
    "    # 3-4-5-6-7\n",
    "    df_above = df.where(df[\"under2\"] == 0)\n",
    "    df_above.dropna(inplace=True)\n",
    "    df_above = df_above.astype(int)  # @TODO c'est bizarre\n",
    "\n",
    "    return df, df_under, df_above\n",
    "\n",
    "\n",
    "def get_X_y(df: pd.DataFrame, target_col=None):\n",
    "    X = df.copy()\n",
    "    if target_col == \"under2\":\n",
    "        X.drop([\"Cover_Type\"], axis=1, inplace=True)\n",
    "    elif target_col == \"Cover_Type\":\n",
    "        X.drop([\"under2\"], axis=1, inplace=True)\n",
    "    if target_col in X.columns:\n",
    "        y = X[target_col]\n",
    "        X.drop([target_col], axis=1, inplace=True)\n",
    "        return X, y\n",
    "    else:\n",
    "        # X.drop([\"under2\", \"Cover_Type\"], axis=1, inplace=True)\n",
    "        return X\n",
    "\n",
    "\n",
    "def train_first_model(X, y, model, test_size=0.25, seed=42, startify=True):\n",
    "    if startify:\n",
    "        strf = y\n",
    "    else:\n",
    "        strf = None\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=strf)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    # model.fit(X_train, y_train)\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_second_model(X, y, model, test_size=0.25, seed=42, startify=True):\n",
    "    if startify:\n",
    "        strf = y\n",
    "    else:\n",
    "        strf = None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=strf)\n",
    "\n",
    "    # le = LabelEncoder()\n",
    "    # y_train = le.fit_transform(y_train)\n",
    "\n",
    "    # model.fit(X, le.fit_transform(y))\n",
    "    model.fit(X_train, y_train)\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred = le.inverse_transform(y_pred)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_stack_clf():\n",
    "    SEED = 42\n",
    "    estimators = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, random_state=SEED)),\n",
    "        # ('svr', make_pipeline(StandardScaler(),\n",
    "        #                       LinearSVC(dual=\"auto\", random_state=SEED))),\n",
    "        ('xgb', xgb.XGBClassifier(n_estimators=200, random_state=SEED)),\n",
    "        # ('xtree'), ExtraTreesClassifier(n_estimators=200, random_state=SEED),\n",
    "        # ('ada', AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=100, random_state=SEED), algorithm=\"SAMME\",n_estimators=200, random_state=SEED)),\n",
    "        ('xgbrf', xgb.XGBRFClassifier(n_estimators=200, random_state=SEED)),\n",
    "    ]\n",
    "    clf = StackingClassifier(\n",
    "        estimators=estimators, final_estimator=LogisticRegression(max_iter=150)\n",
    "    )\n",
    "    return clf\n",
    "\n",
    "def get_stack_clf2():\n",
    "    SEED = 42\n",
    "    estimators = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, random_state=SEED)),\n",
    "        # ('svr', make_pipeline(StandardScaler(),\n",
    "        #                       LinearSVC(dual=\"auto\", random_state=SEED))),\n",
    "        ('xgb', xgb.XGBClassifier(n_estimators=200, random_state=SEED, objective=\"multi:softmax\")),\n",
    "        # ('xtree'), ExtraTreesClassifier(n_estimators=200, random_state=SEED),\n",
    "        # ('ada', AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=100, random_state=SEED), algorithm=\"SAMME\",n_estimators=200, random_state=SEED)),\n",
    "        ('xgbrf', xgb.XGBRFClassifier(n_estimators=200, random_state=SEED, objective=\"multi:softmax\")),\n",
    "    ]\n",
    "    clf = StackingClassifier(\n",
    "        estimators=estimators, final_estimator=LogisticRegression(max_iter=150)\n",
    "    )\n",
    "    return clf\n",
    "\n",
    "def concat_Wilderness_Area(df):\n",
    "    wilderness_areas = [f\"Wilderness_Area{i}\" for i in range(1,5)]\n",
    "    df[\"Wilderness_Area_Synth\"] = df[wilderness_areas] @ range(1,5)\n",
    "    df = df.drop(columns=wilderness_areas)\n",
    "    return df\n",
    "\n",
    "def concat_Soil_Type(df):\n",
    "    soil_types = [f\"Soil_Type{i}\" for i in range(1, 41)]\n",
    "    df[\"Soil_Type_Synth\"] = df[soil_types] @ range(1,41)\n",
    "    df = df.drop(columns=soil_types)\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = concat_Soil_Type(df)\n",
    "    df = concat_Wilderness_Area(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load training data\n",
    "df_train = get_data_train()\n",
    "df_train = preprocess(df_train)\n",
    "\n",
    "# Split the data\n",
    "df_train, df_train_under, df_train_above = split_in_two_df(df_train)\n",
    "X_train_under, y_train_under = get_X_y(\n",
    "    df_train_under, target_col='Cover_Type')\n",
    "X_train_above, y_train_above = get_X_y(\n",
    "    df_train_above, target_col='Cover_Type')\n",
    "X_train, y_train = get_X_y(df_train, target_col='under2')\n",
    "\n",
    "# Train first model\n",
    "# model = xgb.XGBClassifier(n_estimators=200, max_depth=15)\n",
    "# model = get_stack_clf()\n",
    "# model1 = train_first_model(X_train, y_train, model)\n",
    "model1.fit(X_train, y_train, categorical_feature=['Wilderness_Area_Synth', 'Soil_Type_Synth'])\n",
    "\n",
    "# Train second model\n",
    "# model2u = xgb.XGBClassifier(n_estimators=200, max_depth=15)\n",
    "# model2u = get_stack_clf()\n",
    "# model2u = train_second_model(\n",
    "#     X_train_under, y_train_under, model2u)\n",
    "model2u.fit(X_train, y_train, categorical_feature=['Wilderness_Area_Synth', 'Soil_Type_Synth'])\n",
    "\n",
    "# model2a = xgb.XGBClassifier(objective=\"multi:softmax\")\n",
    "# model2a = get_stack_clf2()\n",
    "# model2a = train_second_model(\n",
    "#     X_train_above, y_train_above, model2a)\n",
    "model2a.fit(X_train, y_train, categorical_feature=['Wilderness_Area_Synth', 'Soil_Type_Synth'])\n",
    "\n",
    "# Load Testing data\n",
    "df_test = get_data_test()\n",
    "df_test = preprocess(df_test)\n",
    "print(df_test.shape)\n",
    "\n",
    "X_test = get_X_y(df_test)\n",
    "# X_test = df_test.drop([\"Cover_Type\"], axis=1)\n",
    "# y_test = df_test[\"Cover_Type\"]\n",
    "y1_pred = model1.predict(X_test)\n",
    "\n",
    "X_test[\"under2\"] = y1_pred\n",
    "\n",
    "df_test1, df_test_under, df_test_above = split_in_two_df(X_test)\n",
    "X_test_under = get_X_y(df_test_under)\n",
    "X_test_under.drop([\"under2\"], axis=1, inplace=True)\n",
    "X_test_above = get_X_y(df_test_above)\n",
    "X_test_above.drop([\"under2\"], axis=1, inplace=True)\n",
    "\n",
    "y2u_pred = model2u.predict(X_test_under)\n",
    "y2a_pred = model2a.predict(X_test_above)\n",
    "\n",
    "# Convert prediction into Dataframe\n",
    "pred_under = pd.DataFrame({'Id': X_test_under.Id, 'Cover_Type': y2u_pred})\n",
    "pred_above = pd.DataFrame({'Id': X_test_above.Id, 'Cover_Type': y2a_pred})\n",
    "\n",
    "final_pred = pd.concat([pred_under, pred_above])\n",
    "# accuracy_score(y_test, final_pred)\n",
    "final_pred_clean = clean_predictor(y_pred=final_pred.Cover_Type, Id=final_pred.Id)\n",
    "csv_for_submission(final_pred_clean,\"2model+lgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8748677248677249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.77      0.79       548\n",
      "           2       0.78      0.69      0.73       533\n",
      "           3       0.86      0.88      0.87       520\n",
      "           4       0.98      0.96      0.97       578\n",
      "           5       0.87      0.95      0.91       526\n",
      "           6       0.85      0.90      0.87       547\n",
      "           7       0.96      0.98      0.97       528\n",
      "\n",
      "    accuracy                           0.87      3780\n",
      "   macro avg       0.87      0.87      0.87      3780\n",
      "weighted avg       0.87      0.87      0.87      3780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8751322751322751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.77      0.79       558\n",
      "           2       0.78      0.68      0.72       551\n",
      "           3       0.88      0.88      0.88       514\n",
      "           4       0.97      0.96      0.96       551\n",
      "           5       0.88      0.95      0.91       566\n",
      "           6       0.87      0.92      0.89       530\n",
      "           7       0.94      0.97      0.96       510\n",
      "\n",
      "    accuracy                           0.88      3780\n",
      "   macro avg       0.87      0.88      0.87      3780\n",
      "weighted avg       0.87      0.88      0.87      3780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8637566137566137\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.77      0.78       548\n",
      "           2       0.75      0.67      0.71       531\n",
      "           3       0.84      0.88      0.86       543\n",
      "           4       0.97      0.95      0.96       562\n",
      "           5       0.87      0.93      0.90       508\n",
      "           6       0.88      0.88      0.88       547\n",
      "           7       0.94      0.96      0.95       541\n",
      "\n",
      "    accuracy                           0.86      3780\n",
      "   macro avg       0.86      0.86      0.86      3780\n",
      "weighted avg       0.86      0.86      0.86      3780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8767195767195767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.79      0.81       559\n",
      "           2       0.79      0.72      0.76       548\n",
      "           3       0.86      0.86      0.86       535\n",
      "           4       0.96      0.96      0.96       533\n",
      "           5       0.89      0.94      0.91       540\n",
      "           6       0.86      0.89      0.88       535\n",
      "           7       0.95      0.98      0.96       530\n",
      "\n",
      "    accuracy                           0.88      3780\n",
      "   macro avg       0.88      0.88      0.88      3780\n",
      "weighted avg       0.88      0.88      0.88      3780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/francois/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8613756613756614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.76      0.77       552\n",
      "           2       0.78      0.69      0.73       554\n",
      "           3       0.84      0.87      0.86       543\n",
      "           4       0.97      0.94      0.96       541\n",
      "           5       0.88      0.94      0.91       517\n",
      "           6       0.83      0.87      0.85       542\n",
      "           7       0.94      0.96      0.95       531\n",
      "\n",
      "    accuracy                           0.86      3780\n",
      "   macro avg       0.86      0.86      0.86      3780\n",
      "weighted avg       0.86      0.86      0.86      3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    df_train = get_data_train()\n",
    "    df_train, df_test = train_test_split(df_train, test_size=0.25)\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=31,\n",
    "        num_iterations=100,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model2u = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=31,\n",
    "        num_iterations=100,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model2a = LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=5,\n",
    "        class_weight=c_weights_34567,\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=31,\n",
    "        num_iterations=100,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Split the data\n",
    "    df_train, df_train_under, df_train_above = split_in_two_df(df_train)\n",
    "    X_train_under, y_train_under = get_X_y(\n",
    "        df_train_under, target_col='Cover_Type')\n",
    "    X_train_above, y_train_above = get_X_y(\n",
    "        df_train_above, target_col='Cover_Type')\n",
    "    X_train, y_train = get_X_y(df_train, target_col='under2')\n",
    "\n",
    "    # Train first model\n",
    "    # model = xgb.XGBClassifier(n_estimators=200, max_depth=15)\n",
    "    # model = get_stack_clf()\n",
    "    model1 = train_first_model(X_train, y_train, model)\n",
    "\n",
    "    # Train second model\n",
    "    # model2u = xgb.XGBClassifier(n_estimators=200, max_depth=15)\n",
    "    # model2u = get_stack_clf()\n",
    "    model2u = train_second_model(\n",
    "        X_train_under, y_train_under, model2u)\n",
    "\n",
    "    # model2a = xgb.XGBClassifier(objective=\"multi:softmax\")\n",
    "    # model2a = get_stack_clf2()\n",
    "    model2a = train_second_model(\n",
    "        X_train_above, y_train_above, model2a)\n",
    "\n",
    "    # Load Testing data\n",
    "    # df_test = get_data_test()\n",
    "\n",
    "    # X_test = get_X_y(df_test)\n",
    "    X_test = df_test.drop([\"Cover_Type\"], axis=1)\n",
    "    y_test = df_test[\"Cover_Type\"]\n",
    "    y1_pred = model1.predict(X_test)\n",
    "\n",
    "    X_test[\"under2\"] = y1_pred\n",
    "\n",
    "    df_test1, df_test_under, df_test_above = split_in_two_df(X_test)\n",
    "    X_test_under = get_X_y(df_test_under)\n",
    "    X_test_under.drop([\"under2\"], axis=1, inplace=True)\n",
    "    X_test_above = get_X_y(df_test_above)\n",
    "    X_test_above.drop([\"under2\"], axis=1, inplace=True)\n",
    "\n",
    "    y2u_pred = model2u.predict(X_test_under)\n",
    "    y2a_pred = model2a.predict(X_test_above)\n",
    "\n",
    "    # Convert prediction into Dataframe\n",
    "    pred_under = pd.DataFrame({'Id': X_test_under.Id, 'Cover_Type': y2u_pred})\n",
    "    pred_above = pd.DataFrame({'Id': X_test_above.Id, 'Cover_Type': y2a_pred})\n",
    "\n",
    "    final_pred = pd.concat([pred_under, pred_above])\n",
    "    print(\"Accuracy\",accuracy_score(pd.DataFrame(y_test).sort_index()[\"Cover_Type\"], final_pred.sort_index()[\"Cover_Type\"]))\n",
    "    print(classification_report(pd.DataFrame(y_test).sort_index()[\"Cover_Type\"], final_pred.sort_index()[\"Cover_Type\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8611111111111112"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy\",accuracy_score(pd.DataFrame(y_test).sort_index()[\"Cover_Type\"], final_pred.sort_index()[\"Cover_Type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.75      0.76       560\n",
      "           2       0.77      0.68      0.72       547\n",
      "           3       0.85      0.87      0.86       542\n",
      "           4       0.95      0.95      0.95       535\n",
      "           5       0.89      0.93      0.91       512\n",
      "           6       0.85      0.89      0.87       527\n",
      "           7       0.93      0.97      0.95       557\n",
      "\n",
      "    accuracy                           0.86      3780\n",
      "   macro avg       0.86      0.86      0.86      3780\n",
      "weighted avg       0.86      0.86      0.86      3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pd.DataFrame(y_test).sort_index()[\"Cover_Type\"], final_pred.sort_index()[\"Cover_Type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search pour obtenir les best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split 12 vs 34567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0.9629629629629629\n",
      "i 0.9645502645502646\n",
      "i 0.9677248677248678\n",
      "i 0.9650793650793651\n",
      "i 0.9611111111111111\n",
      "i 0.9674603174603175\n",
      "i 0.9674603174603175\n",
      "i 0.9653439153439154\n",
      "i 0.9616402116402116\n",
      "i 0.9656084656084656\n",
      "Final accuracy 0.965 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# df_train = get_data_train()\n",
    "# kf = KFold(n_splits=10, shuffle=True)\n",
    "acc = []\n",
    "# for i, (train_index, test_index)  in enumerate(kf.split(df_train)):\n",
    "for i in range(10):\n",
    "    model1 = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        is_unbalanced=True,\n",
    "        scale_pos_weight=3,\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=50,\n",
    "        n_estimators=300,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1,\n",
    "        # early_stopping=5\n",
    "    )\n",
    "    df_train = get_data_train()\n",
    "    df_train, df_test = train_test_split(df_train, test_size=0.25)\n",
    "    # df_train, df_val = train_test_split(df_train, test_size=0.25)\n",
    "\n",
    "    # Split the data\n",
    "    df_train, df_train_under, df_train_above = split_in_two_df(df_train)\n",
    "    X_train, y_train = get_X_y(df_train, target_col='under2')\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "    # Train first model\n",
    "    model1.fit(X_train, y_train)#, eval_set=[(X_val, y_val)])#, early_stopping=10)\n",
    "    #Predict\n",
    "    X_test = df_test.drop([\"Cover_Type\"], axis=1)\n",
    "    y_test = (df_test[\"Cover_Type\"]< 3).astype(int)\n",
    "    y1_pred = model1.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test, y1_pred))\n",
    "    print(\"i\", acc[-1])\n",
    "print(f\"Final accuracy {np.mean(acc):.3f} +/- {np.std(acc):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split 1 vs 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0.8518518518518519\n",
      "i 0.85\n",
      "i 0.8611111111111112\n",
      "i 0.8592592592592593\n",
      "i 0.8574074074074074\n",
      "i 0.8555555555555555\n",
      "i 0.8601851851851852\n",
      "i 0.8481481481481481\n",
      "i 0.8694444444444445\n",
      "i 0.8546296296296296\n",
      "Final accuracy 0.857 +/- 0.006\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "# for i, (train_index, test_index)  in enumerate(kf.split(df_train)):\n",
    "for i in range(10):\n",
    "    model2u = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        is_unbalanced=True,\n",
    "        scale_pos_weight=1.3,\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=50,\n",
    "        n_estimators=500,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1,\n",
    "        # early_stopping=5\n",
    "    )\n",
    "\n",
    "    df_train = get_data_train()\n",
    "    # df_train = preprocess(df_train)\n",
    "    df_train, df_train_under, df_train_above = split_in_two_df(df_train)\n",
    "    X_train_under, y_train_under = get_X_y(\n",
    "        df_train_under, target_col='Cover_Type')\n",
    "    y_train_under = y_train_under -1\n",
    "\n",
    "    X_train_under, X_test, y_train_under, y_test = train_test_split(X_train_under, y_train_under, test_size=0.25)\n",
    "\n",
    "    model2u.fit(X_train_under, y_train_under)#, categorical_feature=['Wilderness_Area_Synth', 'Soil_Type_Synth'])\n",
    "\n",
    "    y1_pred = model2u.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test, y1_pred))\n",
    "    print(\"i\", acc[-1])\n",
    "print(f\"Final accuracy {np.mean(acc):.3f} +/- {np.std(acc):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split 3 vs 4 vs 5 vs 6 vs 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0.9514814814814815\n",
      "i 0.9422222222222222\n",
      "i 0.9477777777777778\n",
      "i 0.9437037037037037\n",
      "i 0.9477777777777778\n",
      "i 0.9503703703703704\n",
      "i 0.9403703703703704\n",
      "i 0.947037037037037\n",
      "i 0.9418518518518518\n",
      "i 0.947037037037037\n",
      "Final accuracy 0.946 +/- 0.004\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "# for i, (train_index, test_index)  in enumerate(kf.split(df_train)):\n",
    "for i in range(10):\n",
    "    model2a = LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=5,\n",
    "        class_weight={\n",
    "            3: 0.15,\n",
    "            4: 0.03,\n",
    "            5: 0.15,\n",
    "            6: 0.15,\n",
    "            7: 0.15\n",
    "        },\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=50,\n",
    "        n_estimators=500,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1,\n",
    "        # early_stopping=5\n",
    "    )\n",
    "\n",
    "\n",
    "    df_train = get_data_train()\n",
    "    df_train = preprocess(df_train)\n",
    "    df_train, df_train_under, df_train_above = split_in_two_df(df_train)\n",
    "    X_train_above, y_train_above = get_X_y(\n",
    "        df_train_above, target_col='Cover_Type')\n",
    "    y_train_under = y_train_under -1\n",
    "\n",
    "\n",
    "    X_train_above, X_test, y_train_above, y_test = train_test_split(X_train_above, y_train_above, test_size=0.25)\n",
    "    X_train_above, X_val, y_train_above, y_val = train_test_split(X_train_above, y_train_above, test_size=0.25)\n",
    "\n",
    "\n",
    "    model2a.fit(X_train_above, y_train_above)#, eval_set=[(X_val, y_val)])#, categorical_feature=['Wilderness_Area_Synth', 'Soil_Type_Synth'])\n",
    "\n",
    "    y1_pred = model2a.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test, y1_pred))\n",
    "    print(\"i\", acc[-1])\n",
    "print(f\"Final accuracy {np.mean(acc):.3f} +/- {np.std(acc):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get3model():\n",
    "    model1  = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        is_unbalanced=True,\n",
    "        scale_pos_weight=3,\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=50,\n",
    "        n_estimators=500,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1,\n",
    "        early_stopping=5\n",
    "    )\n",
    "    model2u = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        is_unbalanced=True,\n",
    "        scale_pos_weight=1.3,\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=50,\n",
    "        n_estimators=500,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1,\n",
    "        early_stopping=5\n",
    "    )\n",
    "    model2a = LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=5,\n",
    "        class_weight={\n",
    "            3: 0.15,\n",
    "            4: 0.03,\n",
    "            5: 0.15,\n",
    "            6: 0.15,\n",
    "            7: 0.15\n",
    "        },\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=50,\n",
    "        n_estimators=500,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1,\n",
    "        early_stopping=5\n",
    "    )\n",
    "    return model1, model2u, model2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, model2u, model2a = get3model()\n",
    "\n",
    "# Load training data\n",
    "df_train = get_data_train()\n",
    "df_train = preprocess(df_train)\n",
    "\n",
    "# Split the data\n",
    "df_train, df_train_under, df_train_above = split_in_two_df(df_train)\n",
    "X_train_under, y_train_under = get_X_y(\n",
    "    df_train_under, target_col='Cover_Type')\n",
    "X_train_above, y_train_above = get_X_y(\n",
    "    df_train_above, target_col='Cover_Type')\n",
    "X_train, y_train = get_X_y(df_train, target_col='under2')\n",
    "\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.25)\n",
    "# Train first model\n",
    "model1.fit(X_train, y_train)#, eval_set=[(X_val, y_val)])#, categorical_feature=['Wilderness_Area_Synth', 'Soil_Type_Synth'])\n",
    "\n",
    "# X_train_under, X_val, y_train_under, y_val = train_test_split(X_train_under, y_train_under, test_size=.25)\n",
    "# Train second model\n",
    "model2u.fit(X_train_under, y_train_under)#, eval_set=[(X_val, y_val)])#, categorical_feature=['Wilderness_Area_Synth', 'Soil_Type_Synth'])\n",
    "\n",
    "# X_train_above, X_val, y_train_above, y_val = train_test_split(X_train_above, y_train_above, test_size=.25)\n",
    "model2a.fit(X_train_above, y_train_above)#, eval_set=[(X_val, y_val)])#, categorical_feature=['Wilderness_Area_Synth', 'Soil_Type_Synth'])\n",
    "\n",
    "# Load Testing data\n",
    "df_test = get_data_test()\n",
    "df_test = preprocess(df_test)\n",
    "\n",
    "X_test = get_X_y(df_test)\n",
    "y1_pred = model1.predict(X_test)\n",
    "\n",
    "X_test[\"under2\"] = y1_pred\n",
    "\n",
    "df_test1, df_test_under, df_test_above = split_in_two_df(X_test)\n",
    "X_test_under = get_X_y(df_test_under)\n",
    "X_test_under.drop([\"under2\"], axis=1, inplace=True)\n",
    "X_test_above = get_X_y(df_test_above)\n",
    "X_test_above.drop([\"under2\"], axis=1, inplace=True)\n",
    "\n",
    "y2u_pred = model2u.predict(X_test_under)\n",
    "y2a_pred = model2a.predict(X_test_above)\n",
    "\n",
    "# Convert prediction into Dataframe\n",
    "pred_under = pd.DataFrame({'Id': X_test_under.Id, 'Cover_Type': y2u_pred})\n",
    "pred_above = pd.DataFrame({'Id': X_test_above.Id, 'Cover_Type': y2a_pred})\n",
    "\n",
    "final_pred = pd.concat([pred_under, pred_above])\n",
    "# accuracy_score(y_test, final_pred)\n",
    "final_pred_clean = clean_predictor(y_pred=final_pred.Cover_Type, Id=final_pred.Id)\n",
    "csv_for_submission(final_pred_clean,\"2model + lgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    254536\n",
       "1    210652\n",
       "3     38069\n",
       "7     27269\n",
       "6     23744\n",
       "5     23192\n",
       "4      3550\n",
       "Name: Cover_Type, dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('Output/2model+lgbm.csv')['Cover_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3930414829466814\n",
      "0.32704879737465736\n",
      "2.1672031720335854\n",
      "22.998535407512964\n",
      "3.58121043645486\n",
      "3.5428425083538624\n",
      "3.014845602619385\n"
     ]
    }
   ],
   "source": [
    "deux=253790\n",
    "un=211178\n",
    "trois=38299\n",
    "sept=27531\n",
    "six=23428\n",
    "cinq=23177\n",
    "quatre=3609\n",
    "nb_cl = [211178,253790,38299,3609,23177,23428,27531]\n",
    "c_w ={}\n",
    "for i,e in enumerate(nb_cl):\n",
    "    c_w[i] = np.sum(nb_cl)/(7*e)\n",
    "    print(np.sum(nb_cl)/(7*e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.3930414829466814,\n",
       " 1: 0.32704879737465736,\n",
       " 2: 2.1672031720335854,\n",
       " 3: 22.998535407512964,\n",
       " 4: 3.58121043645486,\n",
       " 5: 3.5428425083538624,\n",
       " 6: 3.014845602619385}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline pour modifier soil type et wilderness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * \n",
    "df_train = get_data_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_Wilderness_Area(df):\n",
    "    wilderness_areas = [f\"Wilderness_Area{i}\" for i in range(1,5)]\n",
    "    df[\"Wilderness_Area_Synth\"] = df[wilderness_areas] @ range(1,5)\n",
    "    df = df.drop(columns=wilderness_areas)\n",
    "    return df\n",
    "\n",
    "def concat_Soil_Type(df):\n",
    "    soil_types = [f\"Soil_Type{i}\" for i in range(1, 41)]\n",
    "    df[\"Soil_Type_Synth\"] = df[soil_types] @ range(1,41)\n",
    "    df = df.drop(columns=soil_types)\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = concat_Soil_Type(df)\n",
    "    df = concat_Wilderness_Area(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_types = [f\"Soil_Type{i}\" for i in range(1, 41)]\n",
    "wilderness_areas = [f\"Wilderness_Area{i}\" for i in range(1,5)]\n",
    "df_test[\"Wilderness_Area_Synth\"] = df_test[wilderness_areas] @ range(1,5)\n",
    "df_train[\"Wilderness_Area_Synth\"] = df_train[wilderness_areas] @ range(1,5)\n",
    "df_test[\"Soil_Type_Synth\"] = df_test[soil_types] @ range(1,41)\n",
    "df_train[\"Soil_Type_Synth\"] = df_train[soil_types] @ range(1,41)\n",
    "df_train = df_train.drop(columns=wilderness_areas + soil_types)\n",
    "df_test = df_test.drop(columns=wilderness_areas + soil_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
